fullnameOverride: promstack
crds:
    enabled: true
    upgradeJob:
        enabled: true
        forceConflicts: true
defaultRules:
    create: false
additionalPrometheusRulesMap:
    node-exporter:
        groups:
            - name: NodeExporter
              rules:
                - alert: HostOutOfMemory
                  expr: sum by (hostname) (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes ) < .10
                  for: 2m
                  labels:
                    severity: critical
                  annotations:
                    summary: Host `{{ $labels.hostname }}` is out of memory
                    description: Memory is at {{ $value | humanizePercentage }} (< 10% left)
                    dashboard: https://grafana.local.mydata247.top/d/rYdddlPWk/node-exporter?var-hostname={{ $labels.hostname }}
                - alert: HostMemoryUnderMemoryPressure
                  expr: sum by (hostname) (rate(node_vmstat_pgmajfault[5m]) > 500)
                  for: 0m
                  labels:
                    severity: critical
                  annotations:
                    summary: Host `{{ $labels.hostname }}` under memory pressure
                    description: Heavy memory pressure, rate of loading memory pages from disk is at {{ $value | humanize }} (pgmajfault > 500/s)
                    dashboard: https://grafana.local.mydata247.top/d/rYdddlPWk/node-exporter?var-hostname={{ $labels.hostname }}
                - alert: HostUnusualNetworkThroughputIn
                  expr: sum by (hostname, device) (rate(node_network_receive_bytes_total[5m]) / node_network_speed_bytes) > .80
                  for: 0m
                  labels:
                    severity: warning
                  annotations:
                    summary: Host `{{ $labels.hostname }}` unusual network Ingress throughput
                    description: Ingress bandwidth is high at {{ $value | humanizePercentage }} (>80%) on interface {{ $labels.device }}
                    dashboard: https://grafana.local.mydata247.top/d/rYdddlPWk/node-exporter?var-hostname={{ $labels.hostname }}
                - alert: HostUnusualNetworkThroughputOut
                  expr: (sum by (hostname, device) (rate(node_network_transmit_bytes_total[5m]) / node_network_speed_bytes)) > .80
                  for: 0m
                  labels:
                    severity: warning
                  annotations:
                    summary: Host `{{ $labels.hostname }}` unusual network Egress throughput
                    description: Egress bandwidth is high at {{ $value | humanizePercentage }} (>80%) on interface {{ $labels.device }}
                    dashboard: https://grafana.local.mydata247.top/d/rYdddlPWk/node-exporter?var-hostname={{ $labels.hostname }}
                - alert: HostUnusualDiskReadRate
                  expr: sum by (hostname, device) (rate(node_disk_io_time_seconds_total[5m])) > .80
                  for: 0m
                  labels:
                    severity: warning
                  annotations:
                    description: Host `{{ $labels.hostname }}` disk is too busy
                    summary: Disk read rate is at {{ $value | humanizePercentage }} (IO wait > 80%) on device {{ $labels.device }}
                    dashboard: https://grafana.local.mydata247.top/d/rYdddlPWk/node-exporter?var-hostname={{ $labels.hostname }}
                    # Please add ignored mountpoints in node_exporter parameters like
                    # "--collector.filesystem.ignored-mount-points=^/(sys|proc|dev|run)($|/)".
                    # Same rule using "node_filesystem_free_bytes" will fire when disk fills for non-root users.
                - alert: HostOutOfDiskSpace
                  expr: sum by (hostname, device)(node_filesystem_avail_bytes{fstype!~"^(fuse.*|tmpfs|cifs|nfs)"} / node_filesystem_size_bytes < 0.15 and on (hostname, device, mountpoint) node_filesystem_readonly)
                  for: 2m
                  labels:
                    severity: warning
                  annotations:
                    summary: Host `{{ $labels.hostname }}` out of disk space
                    description: Disk is almost full at {{ $value | humanizePercentage }} (< 15% left) on device {{ $labels.device }}
                    dashboard: https://grafana.local.mydata247.top/d/rYdddlPWk/node-exporter?var-hostname={{ $labels.hostname }}
                    #      - alert: HostOutOfInodes
                    #   expr: '(node_filesystem_files_free / node_filesystem_files < .10 and ON (hostname, device, mountpoint) node_filesystem_readonly == 0)'
                    #   for: 2m
                    #   labels:
                    #     severity: warning
                    #   annotations:
                    #     summary: Host out of inodes (hostname {{ $labels.hostname }})
                    #     description: "Disk is almost running out of available inodes (< 10% left)"
                    #     dashboard: "https://grafana.local.mydata247.top/d/rYdddlPWk/node-exporter?var-hostname={{ $labels.hostname }}"
                - alert: HostFilesystemDeviceError
                  expr: sum by(hostname, device, mountpoint) (node_filesystem_device_error{fstype!~"^(fuse.*|tmpfs|cifs|nfs)", device!="none"}) > 0
                  for: 2m
                  labels:
                    severity: critical
                  annotations:
                    summary: Host `{{ $labels.hostname }}` has filesystem device error
                    description: Error stat-ing device {{ $labels.device }}, {{ $labels.mountpoint }} filesystem
                    dashboard: https://grafana.local.mydata247.top/d/rYdddlPWk/node-exporter?var-hostname={{ $labels.hostname }}
                    #      - alert: HostUnusualDiskReadLatency
                    #   expr: '(rate(node_disk_read_time_seconds_total[1m]) / rate(node_disk_reads_completed_total[1m]) > 0.1 and rate(node_disk_reads_completed_total[1m]) > 0)'
                    #   for: 2m
                    #   labels:
                    #     severity: warning
                    #   annotations:
                    #     summary: Host unusual disk read latency (hostname {{ $labels.hostname }})
                    #     description: "Disk latency is growing (read operations > 100ms)"
                    #     dashboard: "https://grafana.local.mydata247.top/d/rYdddlPWk/node-exporter?var-hostname={{ $labels.hostname }}"
                    #      - alert: HostUnusualDiskWriteLatency
                    #   expr: '(rate(node_disk_write_time_seconds_total[1m]) / rate(node_disk_writes_completed_total[1m]) > 0.1 and rate(node_disk_writes_completed_total[1m]) > 0)'
                    #   for: 2m
                    #   labels:
                    #     severity: warning
                    #   annotations:
                    #     summary: Host unusual disk write latency (hostname {{ $labels.hostname }})
                    #     description: "Disk latency is growing (write operations > 100ms)"
                    #     dashboard: "https://grafana.local.mydata247.top/d/rYdddlPWk/node-exporter?var-hostname={{ $labels.hostname }}"
                - alert: HostHighCpuLoad
                  expr: sum by (hostname) (1 - (avg without (cpu) (rate(node_cpu_seconds_total{mode="idle"}[5m])))) > .80
                  for: 10m
                  labels:
                    severity: critical
                  annotations:
                    summary: Host `{{ $labels.hostname }}` high CPU load
                    description: CPU load is {{ $value | humanizePercentage }} (>80%)
                    dashboard: https://grafana.local.mydata247.top/d/rYdddlPWk/node-exporter?var-hostname={{ $labels.hostname }}
                - alert: HostCpuStealNoisyNeighbor
                  expr: avg by (hostname) (rate(node_cpu_seconds_total{mode="steal"}[5m])) > .1
                  for: 0m
                  labels:
                    severity: critical
                  annotations:
                    summary: Host `{{ $labels.hostname }}` CPU steal noisy neighbor
                    description: CPU steal is at {{ $value | humanizePercentage }} (>10%). A noisy neighbor is killing VM performances or a spot instance may be out of credit
                    dashboard: https://grafana.local.mydata247.top/d/rYdddlPWk/node-exporter?var-hostname={{ $labels.hostname }}
                - alert: HostCpuHighIowait
                  expr: avg by (hostname) (rate(node_cpu_seconds_total{mode="iowait"}[5m])) > .10
                  for: 0m
                  labels:
                    severity: warning
                  annotations:
                    summary: Host `{{ $labels.hostname }}` CPU high iowait
                    description: CPU iowait is at {{ $value | humanizePercentage }} (>10%). Your CPU is idling waiting for storage to respond.
                    dashboard: https://grafana.local.mydata247.top/d/rYdddlPWk/node-exporter?var-hostname={{ $labels.hostname }}
                    #      - alert: HostUnusualDiskIo
                    #   expr: 'rate(node_disk_io_time_seconds_total[5m]) > 0.8'
                    #   for: 5m
                    #   labels:
                    #     severity: warning
                    #   annotations:
                    #     summary: Host unusual disk IO (hostname {{ $labels.hostname }})
                    #     description: "Disk usage >80%. Check storage for issues or increase IOPS capabilities. Check storage for issues."
                    #     dashboard: "https://grafana.local.mydata247.top/d/rYdddlPWk/node-exporter?var-hostname={{ $labels.hostname }}"
                    # x2 context switches is an arbitrary number.
                    # The alert threshold depends on the nature of the application.
                    # Please read: https://github.com/samber/awesome-prometheus-alerts/issues/58
                - alert: HostContextSwitchingHigh
                  expr: (rate(node_context_switches_total[15m])/count without(mode,cpu) (node_cpu_seconds_total{mode="idle"})) / (rate(node_context_switches_total[1d])/count without(mode,cpu) (node_cpu_seconds_total{mode="idle"})) > 2
                  for: 2m
                  labels:
                    severity: warning
                  annotations:
                    summary: Host {{ $labels.hostname }} context switching high
                    description: Context switching is high (twice the daily average during the last 15m)
                    dashboard: https://grafana.local.mydata247.top/d/rYdddlPWk/node-exporter?var-hostname={{ $labels.hostname }}
                - alert: HostSwapIsFillingUp
                  expr: sum by (hostname) (1 - (node_memory_SwapFree_bytes / node_memory_SwapTotal_bytes)) > .80
                  for: 2m
                  labels:
                    severity: warning
                  annotations:
                    summary: Host `{{ $labels.hostname }}` swap is filling up
                    description: Swap is filling up at {{ $value | humanizePercentage }} (>80%)
                    dashboard: https://grafana.local.mydata247.top/d/rYdddlPWk/node-exporter?var-hostname={{ $labels.hostname }}
                    # NOT COLLECTED
                    #      - alert: HostSystemdServiceCrashed
                    #   expr: '(node_systemd_unit_state{state="failed"} == 1)'
                    #   for: 0m
                    #   labels:
                    #     severity: warning
                    #   annotations:
                    #     summary: Host systemd service crashed (hostname {{ $labels.hostname }})
                    #     description: "systemd service crashed"
                    #     dashboard: "https://grafana.local.mydata247.top/d/rYdddlPWk/node-exporter?var-hostname={{ $labels.hostname }}"
                - alert: HostPhysicalComponentTooHot
                  expr: sum by (hostname, chip) (node_hwmon_temp_celsius > node_hwmon_temp_max_celsius)
                  for: 5m
                  labels:
                    severity: critical
                  annotations:
                    summary: Host `{{ $labels.hostname }}` physical component too hot
                    description: Physical hardware chip {{ $labels.chip }} too hot
                    dashboard: https://grafana.local.mydata247.top/d/rYdddlPWk/node-exporter?var-hostname={{ $labels.hostname }}
                - alert: HostNodeOvertemperatureAlarm
                  expr: sum by (hostname, chip) ((node_hwmon_temp_crit_alarm_celsius > 0) or (node_hwmon_temp_alarm > 0))
                  for: 0m
                  labels:
                    severity: critical
                  annotations:
                    summary: Host `{{ $labels.hostname }}` node overtemperature alarm
                    description: Physical node temperature alarm triggered for chip {{ $labels.chip }}
                    dashboard: https://grafana.local.mydata247.top/d/rYdddlPWk/node-exporter?var-hostname={{ $labels.hostname }}
                    #      - alert: HostSoftwareRaidInsufficientDrives
                    #   expr: '((node_md_disks_required - on(device, instance) node_md_disks{state="active"}) > 0)'
                    #   for: 0m
                    #   labels:
                    #     severity: critical
                    #   annotations:
                    #     summary: Host software RAID insufficient drives (hostname {{ $labels.hostname }})
                    #     description: "MD RAID array {{ $labels.device }} on {{ $labels.hostname }} has insufficient drives remaining."
                    #     dashboard: "https://grafana.local.mydata247.top/d/rYdddlPWk/node-exporter?var-hostname={{ $labels.hostname }}"
                    #      - alert: HostSoftwareRaidDiskFailure
                    #   expr: '(node_md_disks{state="failed"} > 0)'
                    #   for: 2m
                    #   labels:
                    #     severity: warning
                    #   annotations:
                    #     summary: Host software RAID disk failure (hostname {{ $labels.hostname }})
                    #     description: "MD RAID array {{ $labels.device }} on {{ $labels.hostname }} needs attention."
                    #     dashboard: "https://grafana.local.mydata247.top/d/rYdddlPWk/node-exporter?var-hostname={{ $labels.hostname }}"
                    #      - alert: HostKernelVersionDeviations
                    #   expr: 'changes(node_uname_info[1h]) > 0'
                    #   for: 0m
                    #   labels:
                    #     severity: info
                    #   annotations:
                    #     summary: Host kernel version deviations (hostname {{ $labels.hostname }})
                    #     description: "Kernel version for {{ $labels.hostname }} has changed."
                    #     dashboard: "https://grafana.local.mydata247.top/d/rYdddlPWk/node-exporter?var-hostname={{ $labels.hostname }}"
                - alert: HostOomKillDetected
                  expr: sum by (hostname) (increase(node_vmstat_oom_kill[1m]) > 0)
                  for: 0m
                  labels:
                    severity: critical
                  annotations:
                    summary: Host `{{ $labels.hostname }}` OOM kill detected
                    description: OOM kill detected
                    dashboard: https://grafana.local.mydata247.top/d/rYdddlPWk/node-exporter?var-hostname={{ $labels.hostname }}
                    #      - alert: HostEdacCorrectableErrorsDetected
                    #   expr: '(increase(node_edac_correctable_errors_total[1m]) > 0)'
                    #   for: 0m
                    #   labels:
                    #     severity: info
                    #   annotations:
                    #     summary: Host EDAC Correctable Errors detected (hostname {{ $labels.hostname }})
                    #     description: "Host {{ $labels.hostname }} has had {{ printf \"%.0f\" $value }} correctable memory errors reported by EDAC in the last 5 minutes."
                    # dashboard: "https://grafana.local.mydata247.top/d/rYdddlPWk/node-exporter?var-hostname={{ $labels.hostname }}"
                    #      - alert: HostEdacUncorrectableErrorsDetected
                    #   expr: '(node_edac_uncorrectable_errors_total > 0)'
                    #   for: 0m
                    #   labels:
                    #     severity: warning
                    #   annotations:
                    #     summary: Host EDAC Uncorrectable Errors detected (hostname {{ $labels.hostname }})
                    #     description: "Host {{ $labels.hostname }} has had {{ printf \"%.0f\" $value }} uncorrectable memory errors reported by EDAC in the last 5 minutes."
                    #     dashboard: "https://grafana.local.mydata247.top/d/rYdddlPWk/node-exporter?var-hostname={{ $labels.hostname }}"
                - alert: HostNetworkReceiveErrors
                  expr: sum by (hostname, device) (rate(node_network_receive_errs_total[2m]) / rate(node_network_receive_packets_total[2m])) > 0.01
                  for: 2m
                  labels:
                    severity: warning
                  annotations:
                    summary: Host `{{ $labels.hostname }}` Network Receive Errors
                    description: Interface {{ $labels.device }} has encountered {{ $value | humanizePercentage }} (>1%) receive errors in the last 2 minutes
                    dashboard: https://grafana.local.mydata247.top/d/rYdddlPWk/node-exporter?var-hostname={{ $labels.hostname }}
                - alert: HostNetworkTransmitErrors
                  expr: sum by (hostname, device) (rate(node_network_transmit_errs_total[2m]) / rate(node_network_transmit_packets_total[2m])) > 0.01
                  for: 2m
                  labels:
                    severity: warning
                  annotations:
                    summary: Host `{{ $labels.hostname }}` Network Transmit Errors
                    description: Interface {{ $labels.device }} has encountered {{ $value | humanizePercentage }} (>1%) transmit errors in the last 2 minutes
                    dashboard: https://grafana.local.mydata247.top/d/rYdddlPWk/node-exporter?var-hostname={{ $labels.hostname }}
                    #      - alert: HostNetworkBondDegraded
                    #   expr: '((node_bonding_active - node_bonding_slaves) != 0)'
                    #   for: 2m
                    #   labels:
                    #     severity: warning
                    #   annotations:
                    #     summary: Host Network Bond Degraded (hostname {{ $labels.hostname }})
                    #     description: "Bond \"{{ $labels.device }}\" degraded on \"{{ $labels.hostname }}\"."
                    #     dashboard: "https://grafana.local.mydata247.top/d/rYdddlPWk/node-exporter?var-hostname={{ $labels.hostname }}"
                    #      - alert: HostConntrackLimit
                    #   expr: '(node_nf_conntrack_entries / node_nf_conntrack_entries_limit > 0.8)'
                    #   for: 5m
                    #   labels:
                    #     severity: warning
                    #   annotations:
                    #     summary: Host conntrack limit (hostname {{ $labels.hostname }})
                    #     description: "The number of conntrack is approaching limit"
                    #     dashboard: "https://grafana.local.mydata247.top/d/rYdddlPWk/node-exporter?var-hostname={{ $labels.hostname }}"
                - alert: HostClockSkew
                  expr: ((node_timex_offset_seconds > 0.05 and deriv(node_timex_offset_seconds[5m]) >= 0) or (node_timex_offset_seconds < -0.05 and deriv(node_timex_offset_seconds[5m]) <= 0))
                  for: 10m
                  labels:
                    severity: critical
                  annotations:
                    summary: Host `{{ $labels.hostname }}` clock skew
                    description: Clock skew detected. Clock is out of sync. Ensure NTP is configured correctly on this host.
                    dashboard: https://grafana.local.mydata247.top/d/rYdddlPWk/node-exporter?var-hostname={{ $labels.hostname }}
                - alert: HostClockNotSynchronising
                  expr: (min_over_time(node_timex_sync_status[1m]) == 0 and node_timex_maxerror_seconds >= 16)
                  for: 2m
                  labels:
                    severity: critical
                  annotations:
                    summary: Host `{{ $labels.hostname }}` clock not synchronising
                    description: Clock not synchronising. Ensure NTP is configured on this host.
                    dashboard: https://grafana.local.mydata247.top/d/rYdddlPWk/node-exporter?var-hostname={{ $labels.hostname }}
    prometheus-stack:
        groups:
            - name: PromStack
              rules:
                - alert: PrometheusJobMissing
                  expr: absent(up{job="promstack-prometheus"})
                  for: 0m
                  labels:
                    severity: warning
                  annotations:
                    summary: Prometheus job is missing
                    description: A Prometheus job on endpoint {{ $labels.endpoint }} has disappeared
                - alert: PrometheusTargetMissing
                  expr: up == 0
                  for: 0m
                  labels:
                    severity: critical
                  annotations:
                    summary: Prometheus target is missing
                    description: Prometheus job={{ $labels.job }}, hostname={{ $labels.hostname }} has disappeared. An exporter might be crashed.
                # - alert: PrometheusAllTargetsMissing
                #   expr: sum by (job) (up) == 0
                #   for: 0m
                #   labels:
                #     severity: critical
                #   annotations:
                #     summary: Prometheus all targets missing (instance {{ $labels.instance }})
                #     description: "A Prometheus job does not have living target anymore.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
                # - alert: PrometheusTargetMissingWithWarmupTime
                #   expr: sum by (instance, job) ((up == 0) * on (instance) group_left(__name__) (node_time_seconds - node_boot_time_seconds > 600))
                #   for: 0m
                #   labels:
                #     severity: critical
                #   annotations:
                #     summary: Prometheus target missing with warmup time (instance {{ $labels.instance }})
                #     description: "Allow a job time to start up (10 minutes) before alerting that it's down.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
                - alert: PrometheusConfigurationReloadFailure
                  expr: prometheus_config_last_reload_successful != 1
                  for: 0m
                  labels:
                    severity: warning
                  annotations:
                    summary: Prometheus configuration reload failure
                    description: Prometheus configuration reload error
                - alert: PrometheusTooManyRestarts
                  expr: changes(process_start_time_seconds{job=~"^promstack.*"}[10m]) > 2
                  for: 0m
                  labels:
                    severity: warning
                  annotations:
                    summary: Prometheus too many restarts on {{ $labels.job}}
                    description: Prometheus has restarted {{ $value }} times (>2) in the last 10 minutes. It might be crashlooping.
                - alert: PrometheusAlertmanagerJobMissing
                  expr: absent(up{job="promstack-alertmanager"})
                  for: 0m
                  labels:
                    severity: warning
                  annotations:
                    summary: Prometheus AlertManager job missing (instance {{ $labels.instance }})
                    description: A Prometheus AlertManager job on endpoint {{ $labels.endpoint }} has disappeared
                - alert: PrometheusAlertmanagerConfigurationReloadFailure
                  expr: alertmanager_config_last_reload_successful != 1
                  for: 0m
                  labels:
                    severity: warning
                  annotations:
                    summary: Prometheus AlertManager configuration reload failure
                    description: AlertManager configuration reload error
                # - alert: PrometheusAlertmanagerConfigNotSynced
                #   expr: count(count_values("config_hash", alertmanager_config_hash)) > 1
                #   for: 0m
                #   labels:
                #     severity: warning
                #   annotations:
                #     summary: Prometheus AlertManager config not synced (instance {{ $labels.instance }})
                #     description: "Configurations of AlertManager cluster instances are out of sync\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
                - alert: PrometheusAlertmanagerE2eDeadManSwitch
                  expr: vector(1)
                  for: 0m
                  labels:
                    severity: critical
                  annotations:
                    summary: Prometheus AlertManager E2E dead man switch (instance {{ $labels.instance }})
                    description: |-
                        Prometheus DeadManSwitch is an always-firing alert. It's used as an end-to-end test of Prometheus through the Alertmanager.
                          VALUE = {{ $value }}
                          LABELS = {{ $labels }}
                - alert: PrometheusNotConnectedToAlertmanager
                  expr: prometheus_notifications_alertmanagers_discovered < 1
                  for: 0m
                  labels:
                    severity: critical
                  annotations:
                    summary: Prometheus not connected to alertmanager"
                    description: Prometheus cannot connect the alertmanager
                # - alert: PrometheusRuleEvaluationFailures
                #   expr: increase(prometheus_rule_evaluation_failures_total[3m]) > 0
                #   for: 0m
                #   labels:
                #     severity: critical
                #   annotations:
                #     summary: Prometheus rule evaluation failures (instance {{ $labels.instance }})
                #     description: "Prometheus encountered {{ $value }} rule evaluation failures, leading to potentially ignored alerts.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
                # - alert: PrometheusTemplateTextExpansionFailures
                #   expr: increase(prometheus_template_text_expansion_failures_total[3m]) > 0
                #   for: 0m
                #   labels:
                #     severity: critical
                #   annotations:
                #     summary: Prometheus template text expansion failures (instance {{ $labels.instance }})
                #     description: "Prometheus encountered {{ $value }} template text expansion failures\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
                # - alert: PrometheusRuleEvaluationSlow
                #   expr: prometheus_rule_group_last_duration_seconds > prometheus_rule_group_interval_seconds
                #   for: 5m
                #   labels:
                #     severity: warning
                #   annotations:
                #     summary: Prometheus rule evaluation slow (instance {{ $labels.instance }})
                #     description: "Prometheus rule evaluation took more time than the scheduled interval. It indicates a slower storage backend access or too complex query.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
                # - alert: PrometheusNotificationsBacklog
                #   expr: min_over_time(prometheus_notifications_queue_length[10m]) > 0
                #   for: 0m
                #   labels:
                #     severity: warning
                #   annotations:
                #     summary: Prometheus notifications backlog (instance {{ $labels.instance }})
                #     description: "The Prometheus notification queue has not been empty for 10 minutes\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
                # - alert: PrometheusAlertmanagerNotificationFailing
                #   expr: rate(alertmanager_notifications_failed_total[1m]) > 0
                #   for: 0m
                #   labels:
                #     severity: critical
                #   annotations:
                #     summary: Prometheus AlertManager notification failing (instance {{ $labels.instance }})
                #     description: "Alertmanager is failing sending notifications\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
                - alert: PrometheusTargetEmpty
                  expr: prometheus_sd_discovered_targets == 0
                  for: 0m
                  labels:
                    severity: critical
                  annotations:
                    summary: Prometheus target empty
                    description: Prometheus has no target in service discovery on {{ $labels.config }}
                # - alert: PrometheusTargetScrapingSlow
                #   expr: prometheus_target_interval_length_seconds{quantile="0.9"} / on (interval, instance, job) prometheus_target_interval_length_seconds{quantile="0.5"} > 1.05
                #   for: 5m
                #   labels:
                #     severity: warning
                #   annotations:
                #     summary: Prometheus target scraping slow (instance {{ $labels.instance }})
                #     description: "Prometheus is scraping exporters slowly since it exceeded the requested interval time. Your Prometheus server is under-provisioned.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
                # - alert: PrometheusLargeScrape
                #   expr: increase(prometheus_target_scrapes_exceeded_sample_limit_total[10m]) > 10
                #   for: 5m
                #   labels:
                #     severity: warning
                #   annotations:
                #     summary: Prometheus large scrape (instance {{ $labels.instance }})
                #     description: "Prometheus has many scrapes that exceed the sample limit\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
                - alert: PrometheusTargetScrapeDuplicate
                  expr: increase(prometheus_target_scrapes_sample_duplicate_timestamp_total[5m]) > 0
                  for: 0m
                  labels:
                    severity: warning
                  annotations:
                    summary: Prometheus target scrape duplicate
                    description: Prometheus has many samples rejected due to duplicate timestamps but different values
                - alert: PrometheusTsdbCheckpointCreationFailures
                  expr: increase(prometheus_tsdb_checkpoint_creations_failed_total[1m]) > 0
                  for: 0m
                  labels:
                    severity: critical
                  annotations:
                    summary: Prometheus TSDB checkpoint creation failures
                    description: Prometheus encountered {{ $value }} checkpoint creation failures
                - alert: PrometheusTsdbCheckpointDeletionFailures
                  expr: increase(prometheus_tsdb_checkpoint_deletions_failed_total[1m]) > 0
                  for: 0m
                  labels:
                    severity: critical
                  annotations:
                    summary: Prometheus TSDB checkpoint deletion failures
                    description: Prometheus encountered {{ $value }} checkpoint deletion failures
                - alert: PrometheusTsdbCompactionsFailed
                  expr: increase(prometheus_tsdb_compactions_failed_total[1m]) > 0
                  for: 0m
                  labels:
                    severity: critical
                  annotations:
                    summary: Prometheus TSDB compactions failed
                    description: Prometheus encountered {{ $value }} TSDB compactions failures
                - alert: PrometheusTsdbHeadTruncationsFailed
                  expr: increase(prometheus_tsdb_head_truncations_failed_total[1m]) > 0
                  for: 0m
                  labels:
                    severity: critical
                  annotations:
                    summary: Prometheus TSDB head truncations failed
                    description: Prometheus encountered {{ $value }} TSDB head truncation failures
                - alert: PrometheusTsdbReloadFailures
                  expr: increase(prometheus_tsdb_reloads_failures_total[1m]) > 0
                  for: 0m
                  labels:
                    severity: critical
                  annotations:
                    summary: Prometheus TSDB reload failures
                    description: Prometheus encountered {{ $value }} TSDB reload failures
                - alert: PrometheusTsdbWalCorruptions
                  expr: increase(prometheus_tsdb_wal_corruptions_total[1m]) > 0
                  for: 0m
                  labels:
                    severity: critical
                  annotations:
                    summary: Prometheus TSDB WAL corruptions
                    description: Prometheus encountered {{ $value }} TSDB WAL corruptions
                - alert: PrometheusTsdbWalTruncationsFailed
                  expr: increase(prometheus_tsdb_wal_truncations_failed_total[1m]) > 0
                  for: 0m
                  labels:
                    severity: critical
                  annotations:
                    summary: Prometheus TSDB WAL truncations failed
                    description: Prometheus encountered {{ $value }} TSDB WAL truncation failures
                - alert: PrometheusTimeseriesCardinality
                  expr: label_replace(count by(__name__) ({__name__=~".+"}), "name", "$1", "__name__", "(.+)") > 10000
                  for: 0m
                  labels:
                    severity: warning
                  annotations:
                    summary: Prometheus timeseries cardinality explosion
                    description: 'The "{{ $labels.name }}" timeseries cardinality is getting very high: {{ $value }} (>10000)'
alertmanager:
    enabled: true
    config:
        global:
            resolve_timeout: 2m
        route:
            group_by:
                - alertname
                - hostname
            receiver: discord
            group_wait: 30s
            group_interval: 30m
            repeat_interval: 30m
            routes:
                - matchers:
                    - severity =~ "major|critical"
                  receiver: discord-critical
        inhibit_rules:
            - source_matchers:
                - severity = critical
              target_matchers:
                - severity =~ warning|info
              equal:
                - namespace
                - alertname
            - source_matchers:
                - severity = warning
              target_matchers:
                - severity = info
              equal:
                - namespace
                - alertname
            - source_matchers:
                - alertname = InfoInhibitor
              target_matchers:
                - severity = info
              equal:
                - namespace
            - target_matchers:
                - alertname = InfoInhibitor
        receivers:
            - name: discord
              discord_configs:
                - webhook_url: ENC[AES256_GCM,data:CDBw4o3AdwU3Zx0KQX+xkVQNDWxO5cDh0NnFIwgZQXo2NXeUC/c/qBp9S3RjVw96yvV0NPs9ZU0gUdL3hyWGHoBefHq0HUp52nkPZ9tmFGkc8b2Ck89bKOj3RTEa4bnWBt2iv2CepOnRT89ZPD82RrGLyvZOLAdJd9b2G5/e0WuSlGIE1m4JqEh580MpE1azAoNU6nJ1JdXR2dD+nolJarSwIv3JPQiv5/MFtswR8nwWr6ePIrNJuyUYkJ1yoTGDhr7FxDNPDWTycaLI1CYtCfDFVJtP2Gst2eZ6wkOqnQSG2B2XQExQ3mlmOmf1ymLD27gXmbWc/H2jypf1XxvjGqCZgTgyuMNpefW7yAs2wA==,iv:t/CCR6eJHlRCGUN8b6qjHU/FSIawIf3jjNKydxOKTjg=,tag:mvDBOQSBwYWRxvoFkcAIVw==,type:str]
                  send_resolved: true
                  username: Alertmanager
                  avatar_url: https://cdn.jsdelivr.net/gh/homarr-labs/dashboard-icons/png/alertmanager.png
                  title: "{{ if eq .Status \"firing\" }}\U0001F6A8({{ .Alerts.Firing | len }}){{ else }}✅{{ end }} {{ .GroupLabels.alertname }}{{ if .GroupLabels.hostname }} ({{ .GroupLabels.hostname }}){{ end }}"
                  message: "{{ range .Alerts }}\n### {{ .Annotations.summary }}\n**Description:** {{ .Annotations.description }}\n**Severity:** {{ if eq .Labels.severity \"critical\" }}\U0001F6A8{{ else if eq .Labels.severity \"warning\" }}⚠️{{ else }}ℹ️{{ end }} {{ .Labels.severity }}\n**Hostname:** `{{ .Labels.hostname }}`\n**Fired At:** {{ .StartsAt.Format \"2006-01-02 15:04:05 UTC\" }}\n— — — — — — — — — —\n{{- end }}\n[:mag: Query]({{ (index .Alerts 0).GeneratorURL }}) [:bar_chart: Dashboard]({{ (index .Alerts 0).Annotations.dashboard }}) [:no_bell: Silence]({{ template \"__alertmanagerURL\" . }})"
            - name: discord-critical
              discord_configs:
                - webhook_url: ENC[AES256_GCM,data:q4WzuM3+uREhq+3hAtQTdAwbfJsn5b6oxQZXtEsgVV5zGdFFOyL828KV752dJq0rriunaMHetaO8IJc99decdCoWtQ6UJwXprNZ8Ry+oc1+FNq9dXW3FbcfHc9fF5heB/ushyrULGEaKrZEUHmCx/zShEGrcxwfjNmO//YZRyhVTfibNtzoMRtWEe0q9RWHr/7p58tk0xeI/jdtF6gohkcO2NxIY7HsMS57aZeFEDrgftkPY7Xu9FgBEMAcXXQ18r5/wJSUGxjcYnR2recSK/kvxUkMsxelHQCkOI4ESOHeoTYWrfsuKcBoY6jXniw2BlpzRA8rqd92bMhLMSsZ5wULQCZBorp4gQh9pQLAbdA==,iv:IK6S9UbIkVjP7eia8sWGcObMnkGV73faaH1UpNwlAW4=,tag:7Rvmwo9gD6qBDAfxiOEMhw==,type:str]
                  send_resolved: true
                  username: Alertmanager
                  avatar_url: https://cdn.jsdelivr.net/gh/homarr-labs/dashboard-icons/png/alertmanager.png
                  title: "{{ if eq .Status \"firing\" }}\U0001F6A8({{ .Alerts.Firing | len }}){{ else }}✅{{ end }} {{ .GroupLabels.alertname }}{{ if .GroupLabels.hostname }} ({{ .GroupLabels.hostname }}){{ end }}"
                  message: "{{ range .Alerts }}\n### {{ .Annotations.summary }}\n**Description:** {{ .Annotations.description }}\n**Severity:** {{ if eq .Labels.severity \"critical\" }}\U0001F6A8{{ else if eq .Labels.severity \"warning\" }}⚠️{{ else }}ℹ️{{ end }} {{ .Labels.severity }}\n**Hostname:** `{{ .Labels.hostname }}`\n**Fired At:** {{ .StartsAt.Format \"2006-01-02 15:04:05 UTC\" }}\n— — — — — — — — — —\n{{- end }}\n[:mag: Query]({{ (index .Alerts 0).GeneratorURL }}) [:bar_chart: Dashboard]({{ (index .Alerts 0).Annotations.dashboard }}) [:no_bell: Silence]({{ template \"__alertmanagerURL\" . }})"
    ingress:
        enabled: true
        ingressClassName: nginx
        annotations:
            nginx.ingress.kubernetes.io/auth-method: GET
            nginx.ingress.kubernetes.io/auth-url: http://authelia.ingress.svc.cluster.local/api/verify
            nginx.ingress.kubernetes.io/auth-signin: ENC[AES256_GCM,data:BBWpDxME1K43WQMkFue8B2zjcFsid3JAxurNFdV+Ua+O+6mLAfmLV1AYCGp0,iv:UKSDySo8OMtuBg4658ru+TqxD/2IBpxbyjT/pWAZpJY=,tag:6VSRSYutT8i1dTnBEoEJPQ==,type:str]
            nginx.ingress.kubernetes.io/auth-response-headers: Remote-User,Remote-Name,Remote-Groups,Remote-Email
            nginx.ingress.kubernetes.io/auth-snippet: proxy_set_header X-Forwarded-Method $request_method;
        hosts:
            - ENC[AES256_GCM,data:D171khbrqp2LePht6sfII0csZtWzZRJ7JyYfIaSwSQA=,iv:X0G32NjOi2YWXW4zgjSXaFaOWpKKEkB3jtPcYmj67cM=,tag:03cNJMWlvpQLhN4Z9PH1zg==,type:str]
        tls:
            - secretName: local-wildcard-cert
    ## Configuration for creating a ServiceMonitor for AlertManager
    #
    serviceMonitor:
        selfMonitor: true
    alertmanagerSpec:
        logFormat: logfmt
        logLevel: info
        replicas: 1
        retention: 120h
        ## Storage is the definition of how storage will be used by the Alertmanager instances.
        ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/platform/storage.md
        ##
        storage: {}
        # volumeClaimTemplate:
        #   spec:
        #     storageClassName: gluster
        #     accessModes: ["ReadWriteOnce"]
        #     resources:
        #       requests:
        #         storage: 50Gi
        #     selector: {}
        externalUrl: ENC[AES256_GCM,data:xhljnpBXVyFZ0xfDDMLJLXdeKdWoffBZzsAb69CX2nDDZhFtm3IZpw==,iv:DvlQ5KbwVZJOqo7HzYl+qm3wcD0GzBQ/nqbcj5QhJoQ=,tag:/6WjKb5QI+0VsF1qv7sLdQ==,type:str]
        resources:
            requests:
                cpu: 20m
                memory: 64Mi
            limits:
                cpu: 20m
                memory: 128Mi
grafana:
    enabled: false
kubernetesServiceMonitors:
    enabled: true
## Component scraping the kube api server
##
kubeApiServer:
    enabled: true
## Component scraping the kubelet and kubelet-hosted cAdvisor
##
kubelet:
    enabled: true
## Component scraping the kube controller manager
##
kubeControllerManager:
    enabled: false
## Component scraping coreDns. Use either this or kubeDns
##
coreDns:
    enabled: true
## Component scraping etcd
##
kubeEtcd:
    enabled: false
    endpoints: []
## Component scraping kube scheduler
##
kubeScheduler:
    enabled: false
## Component scraping kube proxy
##
kubeProxy:
    enabled: false
## Component scraping kube state metrics
##
kubeStateMetrics:
    enabled: true
## Deploy node exporter as a daemonset to all nodes
##
nodeExporter:
    enabled: true
prometheus-node-exporter:
    prometheus:
        monitor:
            enabled: true
            jobLabel: jobLabel
## Manages Prometheus and Alertmanager components
##
prometheusOperator:
    enabled: true
    kubeletEndpointsEnabled: false
    kubeletEndpointSliceEnabled: true
## Deploy a Prometheus instance
##
prometheus:
    enabled: true
    ingress:
        enabled: true
        ingressClassName: nginx
        annotations:
            nginx.ingress.kubernetes.io/auth-method: GET
            nginx.ingress.kubernetes.io/auth-url: http://authelia.ingress.svc.cluster.local/api/verify
            nginx.ingress.kubernetes.io/auth-signin: ENC[AES256_GCM,data:6pXP+anmtnmMwZofnF3/uVPhxT0v0aTgMh6RZqDxEd5BZlNDeF+HPS7BDaJ/,iv:b6m3M0qpEH0AfF+448NRAXF4NEpP6n00JJ1eo82pBSs=,tag:pacglrUav2HsdI1VWLVjng==,type:str]
            nginx.ingress.kubernetes.io/auth-response-headers: Remote-User,Remote-Name,Remote-Groups,Remote-Email
            nginx.ingress.kubernetes.io/auth-snippet: proxy_set_header X-Forwarded-Method $request_method;
        hosts:
            - ENC[AES256_GCM,data:Ogy4J7II+M4xqaZk9RGh61hPDka9r9mpfhenkrzK,iv:S1+1wXc/JyzD0SedjfSQWJPpKm6ZtMn+041Pc6jCfZU=,tag:kH8YtI+eaX5PIpuG/hn5Ew==,type:str]
        tls:
            - secretName: local-wildcard-cert
    prometheusSpec:
        retention: 10d
        retentionSize: 20GiB
        walCompression: true
        logLevel: info
        resources:
            requests:
                cpu: 100m
                memory: 256Mi
                # limits:
                #   cpu: 2
                #   memory: 4Gi
        storageSpec:
            volumeClaimTemplate:
                spec:
                    storageClassName: longhorn-persistent
                    accessModes:
                        - ReadWriteOnce
                    resources:
                        requests:
                            storage: 25Gi
        ## List of scrape classes to expose to scraping objects such as
        ## PodMonitors, ServiceMonitors, Probes and ScrapeConfigs.
        ##
        scrapeClasses:
            - name: cluster-relabeling
              default: true
              relabelings:
                - action: replace
                  targetLabel: cluster
                  replacement: k3s-cluster
                - action: replace
                  sourceLabels:
                    - __meta_kubernetes_pod_node_name
                  targetLabel: hostname
                # 2. Fallback to endpoint node name if hostname is still empty
                - action: replace
                  sourceLabels:
                    - hostname
                    - __meta_kubernetes_endpoint_node_name
                  separator: ;
                  regex: ^;(.+)$
                  replacement: $1
                  targetLabel: hostname
sops:
    lastmodified: "2026-02-08T16:03:31Z"
    mac: ENC[AES256_GCM,data:Em/B3vkDBCAXzimsFJhNnXAiq168OVXGU0VbNaGrSpvkn7s+meI2+LKp05IrFlvruAMalkKqn8NwAssFW1U+/E+aiZ3b+CVEMdo9Gwvtnzv4v/jy7xUEYAkjhQOaZAOYXQtTvcC6XCpPSKZm9JFl6HnGioxqybMzSfNDYdnWCKc=,iv:15PmUggA2ObNcn9OOaBO4lzXeVd/5wSiJK4b29CbNSg=,tag:a6CvfuBSa+Fv1iwrx93CJw==,type:str]
    pgp:
        - created_at: "2026-02-01T14:23:24Z"
          enc: |-
            -----BEGIN PGP MESSAGE-----

            hQIMA5NT/LvuRqeGAQ/8CUeQUNyMUeVvV9VfyCh+o/uW4U/MVexuzN2j2paYoq3l
            sMtCydafvKMKl3JAsBU4bkCl/Y1ML5CdjGkxmuqtBWPJLIVwrHxojvNRKL3NTdiC
            al3Hh4Q/Y2A0286Vqe+0FH4a2ePMJvfuGbjOSanj3FL2k3D8M8yv8td/cKV6BmTt
            m4yR/pvdF8dkANCdPLvApEcOfJxIp3jEFxKj5vQ1actiG2nrVD2VYvOdBbbCAXEf
            kPzDsMzwi6ZybeeXnCOYzmzT84Y+NxVzkaBk97orn/Zvw4r0qGOOeehnPbO1lou9
            wSFGhO+V/LcOPudIiHVvuFvmnb8Tkwzc5//OFB6DV9Z40E1dBfaWbVJ+D4y1Ni/p
            VF9NDWhFTkx16xx8hCGOADJSpzE/xGRoR0dP59L/LHDoCHT474T5BuEEK/hy85nr
            Yz1J83TIXqzMaxuvXvgDolg5BZDdRZCHcYxR4g+MPXENVEhorVxuUe8itxzcKJ6L
            edeEC+iK5MbsJeGQWFMuYkShJNiJ9qYuPQhNrAy4IKLdG19+rWr8fMqFl8a6ALIt
            Ux8JEo669svg6g/Lkt/ifmk5cTIOpWQN0kFz9F7kjAgi82Fsvvc7KQHVVQ9/XjGO
            8/jo+v+AqZ1EwtUQc6aXW6dgLjKyiXfSM1A4uf/T/mIf8LnSXjmPabytvic83o7U
            aAEJAhAuJRJE219yIg2FUygO/F79rEzgbNqEjrQ28DcxGHlgb1qsbCE3a01dretP
            WPEX559C1enHOzG/Bdy438oCpyzJgAFu7Gx3G7HOYpgL4hAcUXDDXQ6EPTBaHFP8
            81fBEW0lEBhv
            =jX7H
            -----END PGP MESSAGE-----
          fp: 47E4999BED565F9874AA0E7C05DA03D000FC10D1
    encrypted_regex: ^(nginx.ingress.kubernetes.io/auth-signin|webhook_url|hosts|externalUrl)$
    version: 3.11.0
